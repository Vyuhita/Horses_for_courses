%This section explores different existing approaches to optimise Matrix-Mutliplication kernels.
%\subsection{Memory Hierarchy Optimisations}
Traditional approaches that exploited the memory hierarchy include~\cite{van1997summa,low2004api}. They aim at improving the data locality by blocking and loop interchange.
%\subsection{Library based solutions}
Earlier efforts in optimizing matrix multiplication kernels that have primarily focused on library development, are shown in various studies~\cite{anderson1999lapack, goto2008high, low2004api, choi1992scalapack,MKL} and for GPUs there are optimised frameworks such as~\cite{abdelfattah2016kblas}. 
%\subsection{Algorithmic Improvements}
Other studies, including \cite{fatahalian2004understanding, dorrance2014scalable, dalton2015optimizing, nagasaka2017high}, have implemented new algorithms to optimize sparse matrix-matrix and matrix-vector multiplication kernels on GPUs. On the other hand for dense matrix-vector computations we have have~\cite{fujimoto2008dense,nath2011optimizing}  Furthermore,  a GPU and CPU-GPU systems framework has been developed, as outlined in \cite{liu2015framework}. 
%\subsection{Exo-Compilation}
There are compiler and language approaches that address the optimization of the kernels. One of them is exo compilation~\cite{ikarashi2022exocompilation}, in which compiler backend decisions are externalised and are brought into the realm of the programmers. This approach, employs a  Python-like code that rewrites library functions into hardware-level optimised code. This approach is similar to that employed in domain-specific language Halide~\cite{ragan2013halide}, which mainly targets image processing kernels, where matrix-vector operations dominate. Halide targets single-node, multi-node, and GPU-based systems. 
%\subsection{Vectorization}
Vectorisation is also an approach to optimise the performance, some of the works include~\cite{tian2015effective} which presents effective vectorisation techniques on Xeon Phi processors.
%\subsection{Tools}
Tool based approaches reduce the programmer effort by doing automatic dependency inference, parallelism extraction, and/or code generation/execution~\cite{hegde2019d2p, van2009libflame}. ~\cite{hegde2019d2p} only targets Dynamic programming algorithms and ~\cite{van2009libflame} provides an API based approach and developers are required to re-engineer the code using the APIs.
Hierarchical tiling for improved superscalar performance~\cite{carter1995hierarchical} and automatic variable blocking for dense linear algebra algorithms~\cite{gustavson1997recursion} highlight the advanced techniques that improve the execution efficiency of matrix operations by optimising how data is accessed and stored in memory. Further research has been done in~\cite{chatterjee1999nonlinear} on nonlinear array layouts for hierarchical memory systems. Furthermore, an experimental comparison of cache-oblivious and cache-conscious programs demonstrates the benefits of these approaches in parallel algorithm design~\cite{yotov2007experimental}. The automatic generation of block-recursive codes, emphasizing memory optimization, is also studied in~\cite{ahmed2000automatic}.Some studies have compared the energy efficiency of GPUs with FPGAs \cite{mittal2014survey} and examined how the efficiency of matrix multiplication methods varies with the density of multi-GPU systems \cite{zhang2015matrix}. Besides, there is also study on performance evaluation on level-3 operations in CUBLAS \cite{barrachina2008evaluation}.
 Additionally, comparisons between FPGAs and GPUs for memory intensive and task that have less memory usage are shown in \cite{betkaoui2010comparing}.In addition, previous research on scientific and engineering subroutine libraries for vector processors has given important insights that guide modern optimization techniques~\cite{agarwal1989engineering}.
