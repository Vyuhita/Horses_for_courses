Matrix multiplication is a fundamental kernel in domains ranging from machine learning to high-performance computing to benchmarking computer architectures. Developers devote significant time and effort to optimize matrix multiplication kernels. In this paper, we present a systematic study of 
 the performance, speed of development, and energy consumption of multiple flavours of implementations of the matrix multiplication kernel on multicore, manycore, SIMD-, and accelerator- based hardware. We implement hybrid-parallel codes which are a mix of hand-optimized and tool-generated codes and compare them with standard implementations of the matrix multiplication kernel. For various matrix sizes, performance, development effort, and energy consumption of AVX, AVX2, Cilk, OMP, CUDA, and hybrid parallel codes are measured. 
 We implement the recursive matrix multiplication algorithm using task-parallel paradigms available for multicore systems. We make the recursion base case implementation tunable and plug hand-optimized AVX, AVX2, and CUDA code. In some implementation versions, the recursive algorithm is implemented using a code generation tool, which left the recursion base case empty to allow for plugging in of hand-optimized code. We then plug hand-optimized kernel in such implementations. Scalability experiments with different programming models on CPUs, multiple GPUs, Arm server on cloud, and Jetson Nano show that  CUDA implementations not only offer the lowest latency but also are energy efficient in a multi-GPU setting, where the work is distributed. Interestingly, for extended-precision codes, we observe that tool-generated code augmented with hand-optimized CUDA kernel executed on multi-GPU systems performs better than native CUDA code in all aspects of latency, development speed, and energy consumption. 